


# # backend/app/services/tts.py

# import torch
# from transformers import VitsModel, AutoTokenizer
# import io
# import scipy.io.wavfile
# from typing import Optional
# import numpy as np
# from scipy.signal import resample



# # Local setup for Kakao VITS TTS model

# # Load model once when the service starts
# MODEL_NAME = "kakao-enterprise/vits-vctk"
# DEVICE = "cpu"

# print("Loading TTS model...")
# model = VitsModel.from_pretrained(MODEL_NAME).to(DEVICE)
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# print("TTS model loaded successfully!")


# def synthesize_speech(text: str, speaker_id: int = 0, speed: float = 1.0) -> Optional[bytes]:
#     """
#     Generate WAV bytes from text using Kakao VITS.
    
#     Args:
#         text (str): Text to synthesize (dynamically generated by LLM)
#         speaker_id (int): Speaker ID (0-108)
#         speed (float): Playback speed
    
#     Returns:
#         bytes: WAV audio bytes or None on error
#     """
#     # Ensure text is a string
#     if isinstance(text, (list, tuple)):
#         text = " ".join(str(item) for item in text)
    
#     if not isinstance(text, str):
#         text = str(text)
    
#     text = text.strip()
#     if not text:
#         return None

#     try:
#         # Tokenize with proper settings to avoid tensor error
#         inputs = tokenizer(
#             text,
#             return_tensors="pt",
#             padding="max_length",
#             truncation=True,
#             max_length=1000,
#             add_special_tokens=False  # KEY FIX: Prevents tensor creation error
#         )
        
#         inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

#         # Generate audio
#         with torch.no_grad():
#             output = model(**inputs, speaker_id=speaker_id)
#             waveform = output.waveform[0]

#         # Convert to numpy
#         audio_np = waveform.cpu().numpy()
#         audio_np = np.clip(audio_np, -1.0, 1.0)

#         # Adjust speed if needed
#         if speed != 1.0:
#             num_samples = int(len(audio_np) / speed)
#             audio_np = resample(audio_np, num_samples)

#         # Convert to PCM16
#         pcm16 = (audio_np * 32767).astype(np.int16)

#         # Create WAV bytes
#         byte_io = io.BytesIO()
#         scipy.io.wavfile.write(byte_io, model.config.sampling_rate, pcm16)
        
#         return byte_io.getvalue()

#     except Exception as e:
#         print(f"Local TTS error: {e}")
#         return None





# backend/app/services/tts.py

import torch
from transformers import VitsModel, AutoTokenizer
import io
import scipy.io.wavfile
from typing import Optional, List
import numpy as np
from scipy.signal import resample
import re

# Local setup for Kakao VITS TTS


MODEL_NAME = "kakao-enterprise/vits-vctk"
DEVICE = "cpu"

print("Loading TTS model...")
model = VitsModel.from_pretrained(MODEL_NAME).to(DEVICE)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print("TTS model loaded successfully!")



# Helper Functions

def split_text_for_tts(text: str, max_chars: int = 200) -> List[str]:
    """
    Split long text into sentence-based chunks suitable for VITS.
    Prevents truncation and alignment issues.
    """
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current = ""

    for s in sentences:
        if len(current) + len(s) <= max_chars:
            current = f"{current} {s}".strip()
        else:
            if current:
                chunks.append(current)
            current = s

    if current:
        chunks.append(current)

    return chunks


# Main TTS function


def synthesize_speech(
    text: str,
    speaker_id: int = 0,
    speed: float = 1.0
) -> Optional[bytes]:
    """
    Generate WAV bytes from text using Kakao VITS.

    Args:
        text (str): Text to synthesize (LLM output)
        speaker_id (int): Speaker ID (0â€“108)
        speed (float): Playback speed

    Returns:
        bytes: WAV audio bytes or None
    """

    # Normalize input
    if isinstance(text, (list, tuple)):
        text = " ".join(str(x) for x in text)

    text = str(text).strip()
    if not text:
        return None

    try:
        chunks = split_text_for_tts(text)
        audio_segments = []

        for chunk in chunks:
            inputs = tokenizer(
                chunk,
                return_tensors="pt",
                padding=True,
                truncation=False
            )

            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

            with torch.no_grad():
                output = model(**inputs, speaker_id=speaker_id)
                waveform = output.waveform[0]

            audio_np = waveform.cpu().numpy()
            audio_np = np.clip(audio_np, -1.0, 1.0)

            audio_segments.append(audio_np)

        # Merge all chunks
        audio_np = np.concatenate(audio_segments)

        # Adjust speed if needed
        if speed != 1.0:
            num_samples = int(len(audio_np) / speed)
            audio_np = resample(audio_np, num_samples)

        # Convert to PCM16
        pcm16 = (audio_np * 32767).astype(np.int16)

        # Create WAV bytes
        byte_io = io.BytesIO()
        scipy.io.wavfile.write(
            byte_io,
            model.config.sampling_rate,
            pcm16
        )

        return byte_io.getvalue()

    except Exception as e:
        print(f"Local TTS error: {e}")
        return None
